{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "acd92ddc",
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "import os\n",
                "from datasets import load_dataset, load_metric\n",
                "from transformers import PreTrainedTokenizerFast\n",
                "from transformers import RobertaConfig, RobertaModel, RobertaForCausalLM, EncoderDecoderModel\n",
                "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
                "import torch\n",
                "import numpy as np",
            ],
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "7ad71d7a-3d75-4fce-bb3e-87e35f5ef3f0",
            "metadata": {},
            "outputs": [],
            "source": ['os.environ["CUDA_VISIBLE_DEVICES"]="2"'],
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "ac2c25d9-2017-470e-a294-628f4acfb662",
            "metadata": {},
            "outputs": [
                {"name": "stdout", "output_type": "stream", "text": ["cuda\n"]}
            ],
            "source": [
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(device)",
            ],
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "332b6ce2",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Using custom data configuration default-8616eb27344c9d49\n",
                        "Reusing dataset json (/ldap_home/aurelio.prahara/.cache/huggingface/datasets/json/default-8616eb27344c9d49/0.0.0/793d004298099bd3c4e61eb7878475bcf1dc212bf2e34437d85126758720d7f9)\n",
                    ],
                }
            ],
            "source": [
                "raw_datasets = load_dataset('json', data_files={'train': 'en_pt_es_data/train_en_pt.json', 'valid': 'en_pt_es_data/valid_en_pt.json'}, field='data')\n",
                'metric = load_metric("sacrebleu")',
            ],
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "f5be2bf1",
            "metadata": {},
            "outputs": [],
            "source": [
                'en_tokenizer = PreTrainedTokenizerFast(tokenizer_file="en_pt_tokenizers/en_tok.json")\n',
                'es_tokenizer = PreTrainedTokenizerFast(tokenizer_file="en_pt_tokenizers/pt_tok.json")\n',
                "en_tokenizer.unk_token,en_tokenizer.cls_token, en_tokenizer.sep_token,en_tokenizer.pad_token, en_tokenizer.mask_token = ['[UNK]', '[CLS]', '[SEP]', '[PAD]', '[MASK]']\n",
                "es_tokenizer.unk_token, es_tokenizer.cls_token, es_tokenizer.sep_token, es_tokenizer.pad_token, es_tokenizer.mask_token = ['[UNK]', '[CLS]', '[SEP]', '[PAD]', '[MASK]']",
            ],
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "dc827e87",
            "metadata": {"lines_to_next_cell": 1},
            "outputs": [],
            "source": [
                "encoder_max_length = 256\n",
                "decoder_max_length = 256\n",
                "source_lang = 'en'\n",
                "target_lang = 'pt'",
            ],
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "24e6ef45",
            "metadata": {},
            "outputs": [],
            "source": [
                "def process_data_to_model_inputs(batch):\n",
                "  # tokenize the inputs and labels\n",
                '  inputs = [ex[source_lang] for ex in batch["translation"]]\n',
                '  targets = [ex[target_lang] for ex in batch["translation"]]\n',
                '  inputs = en_tokenizer(inputs, padding="max_length", truncation=True, max_length=encoder_max_length)\n',
                '  outputs = es_tokenizer(targets, padding="max_length", truncation=True, max_length=decoder_max_length)\n',
                "\n",
                '  batch["input_ids"] = inputs.input_ids\n',
                '  batch["attention_mask"] = inputs.attention_mask\n',
                '  batch["decoder_input_ids"] = outputs.input_ids\n',
                '  batch["decoder_attention_mask"] = outputs.attention_mask\n',
                '  batch["labels"] = outputs.input_ids.copy()\n',
                "\n",
                "  # because BERT automatically shifts the labels, the labels correspond exactly to `decoder_input_ids`. \n",
                "  # We have to make sure that the PAD token is ignored\n",
                '  batch["labels"] = [[-100 if token == en_tokenizer.pad_token_id else token for token in labels] for labels in batch["labels"]]\n',
                "\n",
                "  return batch",
            ],
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "ac632800",
            "metadata": {"lines_to_next_cell": 2},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Loading cached processed dataset at /ldap_home/aurelio.prahara/.cache/huggingface/datasets/json/default-8616eb27344c9d49/0.0.0/793d004298099bd3c4e61eb7878475bcf1dc212bf2e34437d85126758720d7f9/cache-19994f25b0a05704.arrow\n",
                        "Loading cached processed dataset at /ldap_home/aurelio.prahara/.cache/huggingface/datasets/json/default-8616eb27344c9d49/0.0.0/793d004298099bd3c4e61eb7878475bcf1dc212bf2e34437d85126758720d7f9/cache-1721f295d42c8a04.arrow\n",
                    ],
                }
            ],
            "source": [
                "tokenized_datasets = raw_datasets.map(process_data_to_model_inputs,\n",
                "                                      batched=True,\n",
                "                                      batch_size=256,\n",
                "                                      remove_columns=['translation'])",
            ],
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "b3efcdd7",
            "metadata": {},
            "outputs": [],
            "source": [
                "tokenized_datasets.set_format(\n",
                '    type="torch", columns=["input_ids", "attention_mask", "decoder_input_ids", "decoder_attention_mask", "labels"],\n',
                ")",
            ],
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "7e60920d",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "tensor([[  918,  1567,   610,  ...,  -100,  -100,  -100],\n",
                        "        [ 1545,   109,     0,  ...,  -100,  -100,  -100],\n",
                        "        [22893, 10190,    18,  ...,  -100,  -100,  -100],\n",
                        "        ...,\n",
                        "        [  694, 10128,  5490,  ...,  -100,  -100,  -100],\n",
                        "        [ 1337,  1820,   610,  ...,  -100,  -100,  -100],\n",
                        "        [ 6513,  1688,  2836,  ...,  -100,  -100,  -100]])\n",
                    ],
                }
            ],
            "source": ["print(tokenized_datasets['train']['labels'])"],
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "1d2f1e8a",
            "metadata": {},
            "outputs": [],
            "source": [
                "vocabsize = 30000\n",
                "encoder_config = RobertaConfig(vocab_size = vocabsize,\n",
                "                    max_position_embeddings = 1024,\n",
                "                    num_attention_heads = 8,\n",
                "                    num_hidden_layers = 8,\n",
                "                    hidden_size = 768,\n",
                "                    hidden_dropout_prob=0.2,\n",
                "                    attention_probs_dropout_prob=0.1)\n",
                "encoder = RobertaModel(config=encoder_config)\n",
                "decoder_config = encoder_config\n",
                "decoder_config.add_cross_attention = True\n",
                "decoder_config.is_decoder = True\n",
                "decoder = RobertaForCausalLM(config=decoder_config)\n",
                "model = EncoderDecoderModel(encoder=encoder, decoder=decoder)",
            ],
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "3cb61658",
            "metadata": {"lines_to_next_cell": 2},
            "outputs": [],
            "source": [
                "model.config.decoder_start_token_id = en_tokenizer.cls_token_id\n",
                "model.config.eos_token_id = en_tokenizer.sep_token_id\n",
                "model.config.pad_token_id = en_tokenizer.pad_token_id\n",
                "model.config.vocab_size = model.config.encoder.vocab_size\n",
            ],
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "id": "0ce57ab5-ac23-4093-8bae-895b02a2e94d",
            "metadata": {"scrolled": true, "tags": []},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "EncoderDecoderModel(\n",
                            "  (encoder): RobertaModel(\n",
                            "    (embeddings): RobertaEmbeddings(\n",
                            "      (word_embeddings): Embedding(30000, 768, padding_idx=1)\n",
                            "      (position_embeddings): Embedding(1024, 768, padding_idx=1)\n",
                            "      (token_type_embeddings): Embedding(2, 768)\n",
                            "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
                            "      (dropout): Dropout(p=0.2, inplace=False)\n",
                            "    )\n",
                            "    (encoder): RobertaEncoder(\n",
                            "      (layer): ModuleList(\n",
                            "        (0): RobertaLayer(\n",
                            "          (attention): RobertaAttention(\n",
                            "            (self): RobertaSelfAttention(\n",
                            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
                            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
                            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
                            "              (dropout): Dropout(p=0.1, inplace=False)\n",
                            "            )\n",
                            "            (output): RobertaSelfOutput(\n",
                            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
                            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
                            "              (dropout): Dropout(p=0.2, inplace=False)\n",
                            "            )\n",
                            "          )\n",
                            "          (intermediate): RobertaIntermediate(\n",
                            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
                            "          )\n",
                            "          (output): RobertaOutput(\n",
                            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
                            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
                            "            (dropout): Dropout(p=0.2, inplace=False)\n",
                            "          )\n",
                            "        )\n",
                            "        (1): RobertaLayer(\n",
                            "          (attention): RobertaAttention(\n",
                            "            (self): RobertaSelfAttention(\n",
                            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
                            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
                            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
                            "              (dropout): Dropout(p=0.1, inplace=False)\n",
                            "            )\n",
                            "            (output): RobertaSelfOutput(\n",
                            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
                            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
                            "              (dropout): Dropout(p=0.2, inplace=False)\n",
                            "            )\n",
                            "          )\n",
                            "          (intermediate): RobertaIntermediate(\n",
                            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
                            "          )\n",
                            "          (output): RobertaOutput(\n",
                            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
                            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
                            "            (dropout): Dropout(p=0.2, inplace=False)\n",
                            "          )\n",
                            "        )\n",
                            "        (2): RobertaLayer(\n",
                            "          (attention): RobertaAttention(\n",
                            "            (self): RobertaSelfAttention(\n",
                            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
                            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
                            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
                            "              (dropout): Dropout(p=0.1, inplace=False)\n",
                            "            )\n",
                            "            (output): RobertaSelfOutput(\n",
                            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
                            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
                            "              (dropout): Dropout(p=0.2, inplace=False)\n",
                            "            )\n",
                            "          )\n",
                            "          (intermediate): RobertaIntermediate(\n",
                            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
                            "          )\n",
                            "          (output): RobertaOutput(\n",
                            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
                            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
                            "            (dropout): Dropout(p=0.2, inplace=False)\n",
                            "          )\n",
                            "        )\n",
                            "        (3): RobertaLayer(\n",
                            "          (attention): RobertaAttention(\n",
                            "            (self): RobertaSelfAttention(\n",
                            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
                            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
                            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
                            "              (dropout): Dropout(p=0.1, inplace=False)\n",
                            "            )\n",
                            "            (output): RobertaSelfOutput(\n",
                            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
                            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
                            "              (dropout): Dropout(p=0.2, inplace=False)\n",
                            "            )\n",
                            "          )\n",
                            "          (intermediate): RobertaIntermediate(\n",
                            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
                            "          )\n",
                            "          (output): RobertaOutput(\n",
                            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
                            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
                            "            (dropout): Dropout(p=0.2, inplace=False)\n",
                            "          )\n",
                            "        )\n",
                            "        (4): RobertaLayer(\n",
                            "          (attention): RobertaAttention(\n",
                            "            (self): RobertaSelfAttention(\n",
                            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
                            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
                            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
                            "              (dropout): Dropout(p=0.1, inplace=False)\n",
                            "            )\n",
                            "            (output): RobertaSelfOutput(\n",
                            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
                            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
                            "              (dropout): Dropout(p=0.2, inplace=False)\n",
                            "            )\n",
                            "          )\n",
                            "          (intermediate): RobertaIntermediate(\n",
                            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
                            "          )\n",
                            "          (output): RobertaOutput(\n",
                            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
                            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
                            "            (dropout): Dropout(p=0.2, inplace=False)\n",
                            "          )\n",
                            "        )\n",
                            "        (5): RobertaLayer(\n",
                            "          (attention): RobertaAttention(\n",
                            "            (self): RobertaSelfAttention(\n",
                            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
                            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
                            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
                            "              (dropout): Dropout(p=0.1, inplace=False)\n",
                            "            )\n",
                            "            (output): RobertaSelfOutput(\n",
                            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
                            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
                            "              (dropout): Dropout(p=0.2, inplace=False)\n",
                            "            )\n",
                            "          )\n",
                            "          (intermediate): RobertaIntermediate(\n",
                            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
                            "          )\n",
                            "          (output): RobertaOutput(\n",
                            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
                            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
                            "            (dropout): Dropout(p=0.2, inplace=False)\n",
                            "          )\n",
                            "        )\n",
                            "        (6): RobertaLayer(\n",
                            "          (attention): RobertaAttention(\n",
                            "            (self): RobertaSelfAttention(\n",
                            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
                            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
                            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
                            "              (dropout): Dropout(p=0.1, inplace=False)\n",
                            "            )\n",
                            "            (output): RobertaSelfOutput(\n",
                            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
                            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
                            "              (dropout): Dropout(p=0.2, inplace=False)\n",
                            "            )\n",
                            "          )\n",
                            "          (intermediate): RobertaIntermediate(\n",
                            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
                            "          )\n",
                            "          (output): RobertaOutput(\n",
                            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
                            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
                            "            (dropout): Dropout(p=0.2, inplace=False)\n",
                            "          )\n",
                            "        )\n",
                            "        (7): RobertaLayer(\n",
                            "          (attention): RobertaAttention(\n",
                            "            (self): RobertaSelfAttention(\n",
                            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
                            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
                            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
                            "              (dropout): Dropout(p=0.1, inplace=False)\n",
                            "            )\n",
                            "            (output): RobertaSelfOutput(\n",
                            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
                            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
                            "              (dropout): Dropout(p=0.2, inplace=False)\n",
                            "            )\n",
                            "          )\n",
                            "          (intermediate): RobertaIntermediate(\n",
                            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
                            "          )\n",
                            "          (output): RobertaOutput(\n",
                            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
                            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
                            "            (dropout): Dropout(p=0.2, inplace=False)\n",
                            "          )\n",
                            "        )\n",
                            "      )\n",
                            "    )\n",
                            "    (pooler): RobertaPooler(\n",
                            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
                            "      (activation): Tanh()\n",
                            "    )\n",
                            "  )\n",
                            "  (decoder): RobertaForCausalLM(\n",
                            "    (roberta): RobertaModel(\n",
                            "      (embeddings): RobertaEmbeddings(\n",
                            "        (word_embeddings): Embedding(30000, 768, padding_idx=1)\n",
                            "        (position_embeddings): Embedding(1024, 768, padding_idx=1)\n",
                            "        (token_type_embeddings): Embedding(2, 768)\n",
                            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
                            "        (dropout): Dropout(p=0.2, inplace=False)\n",
                            "      )\n",
                            "      (encoder): RobertaEncoder(\n",
                            "        (layer): ModuleList(\n",
                            "          (0): RobertaLayer(\n",
                            "            (attention): RobertaAttention(\n",
                            "              (self): RobertaSelfAttention(\n",
                            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
                            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
                            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
                            "                (dropout): Dropout(p=0.1, inplace=False)\n",
                            "              )\n",
                            "              (output): RobertaSelfOutput(\n",
                            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
                            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
                            "                (dropout): Dropout(p=0.2, inplace=False)\n",
                            "              )\n",
                            "            )\n",
                            "            (crossattention): RobertaAttention(\n",
                            "              (self): RobertaSelfAttention(\n",
                            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
                            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
                            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
                            "                (dropout): Dropout(p=0.1, inplace=False)\n",
                            "              )\n",
                            "              (output): RobertaSelfOutput(\n",
                            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
                            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
                            "                (dropout): Dropout(p=0.2, inplace=False)\n",
                            "              )\n",
                            "            )\n",
                            "            (intermediate): RobertaIntermediate(\n",
                            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
                            "            )\n",
                            "            (output): RobertaOutput(\n",
                            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
                            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
                            "              (dropout): Dropout(p=0.2, inplace=False)\n",
                            "            )\n",
                            "          )\n",
                            "          (1): RobertaLayer(\n",
                            "            (attention): RobertaAttention(\n",
                            "              (self): RobertaSelfAttention(\n",
                            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
                            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
                            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
                            "                (dropout): Dropout(p=0.1, inplace=False)\n",
                            "              )\n",
                            "              (output): RobertaSelfOutput(\n",
                            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
                            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
                            "                (dropout): Dropout(p=0.2, inplace=False)\n",
                            "              )\n",
                            "            )\n",
                            "            (crossattention): RobertaAttention(\n",
                            "              (self): RobertaSelfAttention(\n",
                            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
                            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
                            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
                            "                (dropout): Dropout(p=0.1, inplace=False)\n",
                            "              )\n",
                            "              (output): RobertaSelfOutput(\n",
                            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
                            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
                            "                (dropout): Dropout(p=0.2, inplace=False)\n",
                            "              )\n",
                            "            )\n",
                            "            (intermediate): RobertaIntermediate(\n",
                            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
                            "            )\n",
                            "            (output): RobertaOutput(\n",
                            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
                            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
                            "              (dropout): Dropout(p=0.2, inplace=False)\n",
                            "            )\n",
                            "          )\n",
                            "          (2): RobertaLayer(\n",
                            "            (attention): RobertaAttention(\n",
                            "              (self): RobertaSelfAttention(\n",
                            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
                            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
                            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
                            "                (dropout): Dropout(p=0.1, inplace=False)\n",
                            "              )\n",
                            "              (output): RobertaSelfOutput(\n",
                            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
                            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
                            "                (dropout): Dropout(p=0.2, inplace=False)\n",
                            "              )\n",
                            "            )\n",
                            "            (crossattention): RobertaAttention(\n",
                            "              (self): RobertaSelfAttention(\n",
                            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
                            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
                            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
                            "                (dropout): Dropout(p=0.1, inplace=False)\n",
                            "              )\n",
                            "              (output): RobertaSelfOutput(\n",
                            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
                            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
                            "                (dropout): Dropout(p=0.2, inplace=False)\n",
                            "              )\n",
                            "            )\n",
                            "            (intermediate): RobertaIntermediate(\n",
                            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
                            "            )\n",
                            "            (output): RobertaOutput(\n",
                            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
                            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
                            "              (dropout): Dropout(p=0.2, inplace=False)\n",
                            "            )\n",
                            "          )\n",
                            "          (3): RobertaLayer(\n",
                            "            (attention): RobertaAttention(\n",
                            "              (self): RobertaSelfAttention(\n",
                            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
                            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
                            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
                            "                (dropout): Dropout(p=0.1, inplace=False)\n",
                            "              )\n",
                            "              (output): RobertaSelfOutput(\n",
                            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
                            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
                            "                (dropout): Dropout(p=0.2, inplace=False)\n",
                            "              )\n",
                            "            )\n",
                            "            (crossattention): RobertaAttention(\n",
                            "              (self): RobertaSelfAttention(\n",
                            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
                            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
                            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
                            "                (dropout): Dropout(p=0.1, inplace=False)\n",
                            "              )\n",
                            "              (output): RobertaSelfOutput(\n",
                            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
                            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
                            "                (dropout): Dropout(p=0.2, inplace=False)\n",
                            "              )\n",
                            "            )\n",
                            "            (intermediate): RobertaIntermediate(\n",
                            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
                            "            )\n",
                            "            (output): RobertaOutput(\n",
                            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
                            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
                            "              (dropout): Dropout(p=0.2, inplace=False)\n",
                            "            )\n",
                            "          )\n",
                            "          (4): RobertaLayer(\n",
                            "            (attention): RobertaAttention(\n",
                            "              (self): RobertaSelfAttention(\n",
                            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
                            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
                            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
                            "                (dropout): Dropout(p=0.1, inplace=False)\n",
                            "              )\n",
                            "              (output): RobertaSelfOutput(\n",
                            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
                            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
                            "                (dropout): Dropout(p=0.2, inplace=False)\n",
                            "              )\n",
                            "            )\n",
                            "            (crossattention): RobertaAttention(\n",
                            "              (self): RobertaSelfAttention(\n",
                            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
                            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
                            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
                            "                (dropout): Dropout(p=0.1, inplace=False)\n",
                            "              )\n",
                            "              (output): RobertaSelfOutput(\n",
                            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
                            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
                            "                (dropout): Dropout(p=0.2, inplace=False)\n",
                            "              )\n",
                            "            )\n",
                            "            (intermediate): RobertaIntermediate(\n",
                            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
                            "            )\n",
                            "            (output): RobertaOutput(\n",
                            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
                            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
                            "              (dropout): Dropout(p=0.2, inplace=False)\n",
                            "            )\n",
                            "          )\n",
                            "          (5): RobertaLayer(\n",
                            "            (attention): RobertaAttention(\n",
                            "              (self): RobertaSelfAttention(\n",
                            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
                            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
                            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
                            "                (dropout): Dropout(p=0.1, inplace=False)\n",
                            "              )\n",
                            "              (output): RobertaSelfOutput(\n",
                            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
                            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
                            "                (dropout): Dropout(p=0.2, inplace=False)\n",
                            "              )\n",
                            "            )\n",
                            "            (crossattention): RobertaAttention(\n",
                            "              (self): RobertaSelfAttention(\n",
                            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
                            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
                            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
                            "                (dropout): Dropout(p=0.1, inplace=False)\n",
                            "              )\n",
                            "              (output): RobertaSelfOutput(\n",
                            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
                            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
                            "                (dropout): Dropout(p=0.2, inplace=False)\n",
                            "              )\n",
                            "            )\n",
                            "            (intermediate): RobertaIntermediate(\n",
                            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
                            "            )\n",
                            "            (output): RobertaOutput(\n",
                            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
                            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
                            "              (dropout): Dropout(p=0.2, inplace=False)\n",
                            "            )\n",
                            "          )\n",
                            "          (6): RobertaLayer(\n",
                            "            (attention): RobertaAttention(\n",
                            "              (self): RobertaSelfAttention(\n",
                            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
                            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
                            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
                            "                (dropout): Dropout(p=0.1, inplace=False)\n",
                            "              )\n",
                            "              (output): RobertaSelfOutput(\n",
                            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
                            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
                            "                (dropout): Dropout(p=0.2, inplace=False)\n",
                            "              )\n",
                            "            )\n",
                            "            (crossattention): RobertaAttention(\n",
                            "              (self): RobertaSelfAttention(\n",
                            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
                            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
                            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
                            "                (dropout): Dropout(p=0.1, inplace=False)\n",
                            "              )\n",
                            "              (output): RobertaSelfOutput(\n",
                            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
                            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
                            "                (dropout): Dropout(p=0.2, inplace=False)\n",
                            "              )\n",
                            "            )\n",
                            "            (intermediate): RobertaIntermediate(\n",
                            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
                            "            )\n",
                            "            (output): RobertaOutput(\n",
                            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
                            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
                            "              (dropout): Dropout(p=0.2, inplace=False)\n",
                            "            )\n",
                            "          )\n",
                            "          (7): RobertaLayer(\n",
                            "            (attention): RobertaAttention(\n",
                            "              (self): RobertaSelfAttention(\n",
                            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
                            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
                            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
                            "                (dropout): Dropout(p=0.1, inplace=False)\n",
                            "              )\n",
                            "              (output): RobertaSelfOutput(\n",
                            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
                            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
                            "                (dropout): Dropout(p=0.2, inplace=False)\n",
                            "              )\n",
                            "            )\n",
                            "            (crossattention): RobertaAttention(\n",
                            "              (self): RobertaSelfAttention(\n",
                            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
                            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
                            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
                            "                (dropout): Dropout(p=0.1, inplace=False)\n",
                            "              )\n",
                            "              (output): RobertaSelfOutput(\n",
                            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
                            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
                            "                (dropout): Dropout(p=0.2, inplace=False)\n",
                            "              )\n",
                            "            )\n",
                            "            (intermediate): RobertaIntermediate(\n",
                            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
                            "            )\n",
                            "            (output): RobertaOutput(\n",
                            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
                            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
                            "              (dropout): Dropout(p=0.2, inplace=False)\n",
                            "            )\n",
                            "          )\n",
                            "        )\n",
                            "      )\n",
                            "    )\n",
                            "    (lm_head): RobertaLMHead(\n",
                            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
                            "      (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
                            "      (decoder): Linear(in_features=768, out_features=30000, bias=True)\n",
                            "    )\n",
                            "  )\n",
                            ")",
                        ]
                    },
                    "execution_count": 13,
                    "metadata": {},
                    "output_type": "execute_result",
                }
            ],
            "source": ["model.to(device)"],
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "id": "bcab0ce4",
            "metadata": {},
            "outputs": [],
            "source": [
                "batch_size = 28\n",
                "args = Seq2SeqTrainingArguments(\n",
                '    output_dir="./roberta_pt_checkpoints",\n',
                '    evaluation_strategy="steps",\n',
                "    eval_steps=2000,\n",
                "    per_device_train_batch_size=batch_size,\n",
                "    per_device_eval_batch_size=batch_size,\n",
                "    # learning_rate=6e-4,\n",
                "    weight_decay=1e-5,\n",
                "    max_grad_norm=1.0,\n",
                "    warmup_steps=4000,\n",
                "    num_train_epochs=50,\n",
                "    # label_smoothing_factor=0.1,\n",
                "    predict_with_generate=True,\n",
                "    fp16=True,\n",
                '    save_strategy="epoch",\n',
                "    save_total_limit=10,\n",
                "    adafactor=True\n",
                ")",
            ],
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "id": "ed86a743",
            "metadata": {},
            "outputs": [],
            "source": [
                "def postprocess_text(preds, labels):\n",
                "    preds = [pred.strip() for pred in preds]\n",
                "    labels = [[label.strip()] for label in labels]\n",
                "\n",
                "    return preds, labels",
            ],
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "id": "2db15615",
            "metadata": {"lines_to_next_cell": 1},
            "outputs": [],
            "source": [
                "def compute_metrics(eval_preds):\n",
                "    labels_ids = eval_preds.label_ids\n",
                "    pred_ids = eval_preds.predictions\n",
                "    pred_str = es_tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
                "    labels_ids[labels_ids == -100] = es_tokenizer.pad_token_id\n",
                "    label_str = es_tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
                "\n",
                "    # postprocessing\n",
                "    pred_str, label_str = postprocess_text(pred_str, label_str)\n",
                "\n",
                "    result = metric.compute(predictions=pred_str, references=label_str)\n",
                '    result = {"bleu": result["score"]}\n',
                "\n",
                "    prediction_lens = [np.count_nonzero(pred != es_tokenizer.pad_token_id) for pred in pred_ids]\n",
                '    result["gen_len"] = np.mean(prediction_lens)\n',
                "    result = {k: round(v, 4) for k, v in result.items()}\n",
                "    return result",
            ],
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "id": "c7fa02c0",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": ["Using amp fp16 backend\n"],
                }
            ],
            "source": [
                "trainer = Seq2SeqTrainer(\n",
                "    model=model,\n",
                "    args=args,\n",
                '    train_dataset=tokenized_datasets["train"],\n',
                '    eval_dataset=tokenized_datasets["valid"],\n',
                "    tokenizer=en_tokenizer,\n",
                "    compute_metrics=compute_metrics\n",
                ")",
            ],
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "id": "4e080639",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "***** Running training *****\n",
                        "  Num examples = 461640\n",
                        "  Num Epochs = 50\n",
                        "  Instantaneous batch size per device = 28\n",
                        "  Total train batch size (w. parallel, distributed & accumulation) = 56\n",
                        "  Gradient Accumulation steps = 1\n",
                        "  Total optimization steps = 412200\n",
                        "/ldap_home/aurelio.prahara/.conda/envs/huggingface/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
                        "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
                    ],
                },
                {
                    "data": {
                        "text/html": [
                            "\n",
                            "    <div>\n",
                            "      \n",
                            "      <progress value='2069' max='412200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
                            "      [  2069/412200 24:14 < 80:11:15, 1.42 it/s, Epoch 0.25/50]\n",
                            "    </div>\n",
                            '    <table border="1" class="dataframe">\n',
                            "  <thead>\n",
                            '    <tr style="text-align: left;">\n',
                            "      <th>Step</th>\n",
                            "      <th>Training Loss</th>\n",
                            "      <th>Validation Loss</th>\n",
                            "      <th>Bleu</th>\n",
                            "      <th>Gen Len</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <td>2000</td>\n",
                            "      <td>2.922800</td>\n",
                            "      <td>2.558300</td>\n",
                            "      <td>5.123900</td>\n",
                            "      <td>20.000000</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table><p>",
                        ],
                        "text/plain": ["<IPython.core.display.HTML object>"],
                    },
                    "metadata": {},
                    "output_type": "display_data",
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "***** Running Evaluation *****\n",
                        "  Num examples = 7226\n",
                        "  Batch size = 56\n",
                    ],
                },
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                        "\u001b[0;32m<ipython-input-18-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
                        "\u001b[0;32m~/.conda/envs/huggingface/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, **kwargs)\u001b[0m\n\u001b[1;32m   1267\u001b[0m                         \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1269\u001b[0;31m                     \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1270\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_flos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating_point_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m~/.conda/envs/huggingface/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1758\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_amp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1759\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1760\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1761\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1762\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m~/.conda/envs/huggingface/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   1792\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1793\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1794\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1795\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1796\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m~/.conda/envs/huggingface/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m~/.conda/envs/huggingface/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m~/.conda/envs/huggingface/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mreplicate\u001b[0;34m(self, module, device_ids)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        '\u001b[0;32m~/.conda/envs/huggingface/lib/python3.8/site-packages/torch/nn/parallel/replicate.py\u001b[0m in \u001b[0;36mreplicate\u001b[0;34m(network, devices, detach)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetach\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_replicatable_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         raise RuntimeError("Cannot replicate network where python modules are "\n\u001b[1;32m     81\u001b[0m                            "childrens of ScriptModule")\n',
                        "\u001b[0;32m~/.conda/envs/huggingface/lib/python3.8/site-packages/torch/nn/parallel/replicate.py\u001b[0m in \u001b[0;36m_replicatable_module\u001b[0;34m(module, memo)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_replicatable_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m~/.conda/envs/huggingface/lib/python3.8/site-packages/torch/nn/parallel/replicate.py\u001b[0m in \u001b[0;36m_replicatable_module\u001b[0;34m(module, memo)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_replicatable_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m~/.conda/envs/huggingface/lib/python3.8/site-packages/torch/nn/parallel/replicate.py\u001b[0m in \u001b[0;36m_replicatable_module\u001b[0;34m(module, memo)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_replicatable_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m~/.conda/envs/huggingface/lib/python3.8/site-packages/torch/nn/parallel/replicate.py\u001b[0m in \u001b[0;36m_replicatable_module\u001b[0;34m(module, memo)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_replicatable_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m~/.conda/envs/huggingface/lib/python3.8/site-packages/torch/nn/parallel/replicate.py\u001b[0m in \u001b[0;36m_replicatable_module\u001b[0;34m(module, memo)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_replicatable_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m~/.conda/envs/huggingface/lib/python3.8/site-packages/torch/nn/parallel/replicate.py\u001b[0m in \u001b[0;36m_replicatable_module\u001b[0;34m(module, memo)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_replicatable_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m~/.conda/envs/huggingface/lib/python3.8/site-packages/torch/nn/parallel/replicate.py\u001b[0m in \u001b[0;36m_replicatable_module\u001b[0;34m(module, memo)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_replicatable_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m~/.conda/envs/huggingface/lib/python3.8/site-packages/torch/nn/parallel/replicate.py\u001b[0m in \u001b[0;36m_replicatable_module\u001b[0;34m(module, memo)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_replicatable_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m~/.conda/envs/huggingface/lib/python3.8/site-packages/torch/nn/parallel/replicate.py\u001b[0m in \u001b[0;36m_replicatable_module\u001b[0;34m(module, memo)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_jit_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmemo\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m~/.conda/envs/huggingface/lib/python3.8/site-packages/torch/nn/parallel/replicate.py\u001b[0m in \u001b[0;36m_is_jit_enabled\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_is_jit_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
                    ],
                },
            ],
            "source": ["trainer.train()"],
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "930f72ed-ff43-42d8-b987-76fd1e49d4dc",
            "metadata": {},
            "outputs": [],
            "source": [],
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d0f3b5fb-30dc-4da6-8754-f6b42bf4c0cd",
            "metadata": {},
            "outputs": [],
            "source": [],
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "810240b6-53c0-49cc-80f4-1138403df742",
            "metadata": {},
            "outputs": [],
            "source": [],
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "613ac3d4-fb7b-4401-8140-dbcd7127b6b4",
            "metadata": {},
            "outputs": [],
            "source": [],
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7d1493e6-6cbd-4432-9997-de035d38c27f",
            "metadata": {},
            "outputs": [],
            "source": [],
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "499ceeae-efe6-48b2-8649-b756fd8d68ef",
            "metadata": {},
            "outputs": [],
            "source": [],
        },
    ],
    "metadata": {
        "jupytext": {
            "cell_metadata_filter": "-all",
            "main_language": "python",
            "notebook_metadata_filter": "-all",
        },
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3",
        },
        "language_info": {
            "codemirror_mode": {"name": "ipython", "version": 3},
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10",
        },
    },
    "nbformat": 4,
    "nbformat_minor": 5,
}
