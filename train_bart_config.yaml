train_data_dir: indomain_data / trial_en_es_train.json
valid_data_dir: indomain_data / trial_en_es_valid.json

source_lang: en
target_lang: es

source_tokenizer: bpe_en_es / en_tok.json
target_tokenizer: bpe_en_es / es_tok.json

encoder_max_length: 256
decoder_max_length: 256

metric: sacrebleu

# Model Config

vocab_size: 30000
dropout: 0.1
attention_dropout: 0.1
encoder_layers: 7
decoder_layers: 7
encoder_attention_heads: 8
decoder_attention_heads: 8
activation_function: gelu
max_position_embeddings: 1024
encoder_ffn_dim: 1024
decoder_ffn_dim: 1024

# Training Config

output_dir: bart_en_es
batch_size: 28
evaluation_strategy: steps
eval_steps: 500
learning_rate: 6e-5
warmup_steps: 4000
label_smoothing_factor: 0.1
weight_decay: 0.1
max_grad_norm: 1.0
num_train_epochs: 50
lr_scheduler_type: linear
save_strategy: steps
save_steps: 2000
save_total_limit: 10
seed: 42
fp16: True
adafactor: True
predict_with_generate: True
